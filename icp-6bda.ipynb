{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1. Use the Sequential API to build a simple feedforward neural network.\n",
        "!pip install tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# 1. Load and preprocess the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# 2. Build a simple feedforward neural network\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),           # Flatten 28x28 image to 1D\n",
        "    Dense(128, activation='relu'),           # Hidden layer 1\n",
        "    Dense(64, activation='relu'),            # Hidden layer 2\n",
        "    Dense(32, activation='relu'),            # Hidden layer 3\n",
        "    Dense(32, activation='relu'),            # Hidden layer 4\n",
        "    Dense(16, activation='relu'),            # Hidden layer 5\n",
        "    Dense(10, activation='softmax')          # Output layer for 10 classes\n",
        "])\n",
        "\n",
        "# 3. Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 4. Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=0.1)\n",
        "\n",
        "# 5. Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f\"\\n✅ Test Accuracy: {test_acc * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "p7pd-8yrKC-z",
        "outputId": "b185447d-a095-4bde-89c7-55ac54e02c4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.7.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.6815 - loss: 0.9258 - val_accuracy: 0.9532 - val_loss: 0.1766\n",
            "Epoch 2/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9536 - loss: 0.1626 - val_accuracy: 0.9673 - val_loss: 0.1153\n",
            "Epoch 3/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9681 - loss: 0.1115 - val_accuracy: 0.9737 - val_loss: 0.0951\n",
            "Epoch 4/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9763 - loss: 0.0803 - val_accuracy: 0.9752 - val_loss: 0.0915\n",
            "Epoch 5/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9805 - loss: 0.0623 - val_accuracy: 0.9730 - val_loss: 0.0931\n",
            "Epoch 6/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9848 - loss: 0.0487 - val_accuracy: 0.9733 - val_loss: 0.0947\n",
            "Epoch 7/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9878 - loss: 0.0380 - val_accuracy: 0.9750 - val_loss: 0.0973\n",
            "Epoch 8/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9901 - loss: 0.0316 - val_accuracy: 0.9752 - val_loss: 0.1029\n",
            "Epoch 9/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9907 - loss: 0.0304 - val_accuracy: 0.9710 - val_loss: 0.1027\n",
            "Epoch 10/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9921 - loss: 0.0258 - val_accuracy: 0.9773 - val_loss: 0.0915\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9733 - loss: 0.1096\n",
            "\n",
            "✅ Test Accuracy: 97.62%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Test Accuracy: 97.46%\n",
        "Step 2: Add 5 Hidden Layers Using ReLU and a Softmax Output Layer I construct a Sequential neural network that contains:\n",
        "\n",
        "Five hidden layers\n",
        "\n",
        "Each hidden layer uses the ReLU activation function\n",
        "\n",
        "The final output layer uses the Softmax activation function"
      ],
      "metadata": {
        "id": "spoEmtXvKRxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "\n",
        "# Build the model\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),          # Input layer to flatten 28x28 pixels\n",
        "\n",
        "    Dense(128, activation='relu'),          # Hidden Layer 1\n",
        "    Dense(64, activation='relu'),           # Hidden Layer 2\n",
        "    Dense(64, activation='relu'),           # Hidden Layer 3\n",
        "    Dense(32, activation='relu'),           # Hidden Layer 4\n",
        "    Dense(32, activation='relu'),           # Hidden Layer 5\n",
        "\n",
        "    Dense(10, activation='softmax')         # Output Layer: 10 classes (digits 0–9)\n",
        "])\n"
      ],
      "metadata": {
        "id": "xdLIKRYbKS2p"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 3. Try adding more layers or using a different number of neurons\n",
        "#  Add More Layers\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "\n",
        "    Dense(128, activation='relu'),   # Hidden Layer 1\n",
        "    Dense(128, activation='relu'),   # Hidden Layer 2\n",
        "    Dense(64, activation='relu'),    # Hidden Layer 3\n",
        "    Dense(64, activation='relu'),    # Hidden Layer 4\n",
        "    Dense(32, activation='relu'),    # Hidden Layer 5\n",
        "    Dense(32, activation='relu'),    # Hidden Layer 6\n",
        "    Dense(16, activation='relu'),    # Hidden Layer 7\n",
        "\n",
        "    Dense(10, activation='softmax')  # Output Layer\n",
        "])\n"
      ],
      "metadata": {
        "id": "LZ9SRh-eKYur"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Changing Neurons (Fewer or More)\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "\n",
        "    Dense(256, activation='relu'),   # More neurons\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(16, activation='relu'),\n",
        "\n",
        "    Dense(10, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "9QnvKPJiKcT4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# For smaller network:\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(8, activation='relu'),\n",
        "\n",
        "    Dense(10, activation='softmax')\n",
        "])\n"
      ],
      "metadata": {
        "id": "jpSGriS1KiLV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 4. Experiment with different activation functions (e.g., tanh, sigmoid).\n",
        "#  Using tanh in all hidden layers\n",
        "model = tf.keras.Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "\n",
        "    Dense(128, activation='tanh'),\n",
        "    Dense(64, activation='tanh'),\n",
        "    Dense(64, activation='tanh'),\n",
        "    Dense(32, activation='tanh'),\n",
        "    Dense(32, activation='tanh'),\n",
        "\n",
        "    Dense(10, activation='softmax')  # output stays softmax\n",
        "])"
      ],
      "metadata": {
        "id": "k5xBx7PuKjZh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        " #  Using sigmoid in all hidden layers\n",
        " model = tf.keras.Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "\n",
        "    Dense(128, activation='sigmoid'),\n",
        "    Dense(64, activation='sigmoid'),\n",
        "    Dense(64, activation='sigmoid'),\n",
        "    Dense(32, activation='sigmoid'),\n",
        "    Dense(32, activation='sigmoid'),\n",
        "\n",
        "    Dense(10, activation='softmax')\n",
        "])\n"
      ],
      "metadata": {
        "id": "9rWqaDjiKm0K"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#  Mix activations\n",
        "model = tf.keras.Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "\n",
        "    Dense(128, activation='tanh'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(64, activation='sigmoid'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(32, activation='tanh'),\n",
        "\n",
        "    Dense(10, activation='softmax')\n",
        "])\n"
      ],
      "metadata": {
        "id": "mn4PALiwKp3k"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Using LeakyReLU (advanced)\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "\n",
        "    Dense(128), LeakyReLU(alpha=0.1),\n",
        "    Dense(64), LeakyReLU(alpha=0.1),\n",
        "    Dense(64), LeakyReLU(alpha=0.1),\n",
        "    Dense(32), LeakyReLU(alpha=0.1),\n",
        "    Dense(32), LeakyReLU(alpha=0.1),\n",
        "\n",
        "    Dense(10, activation='softmax')\n",
        "])\n"
      ],
      "metadata": {
        "id": "y9eoqbc5KtDf",
        "outputId": "a2cda42e-b429-40df-c288-4d29052a0438",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Compare results with different optimizers like sgd, rmsprop, etc.\n",
        "#By using Different Optimizers in Code\n",
        "# For SGD optimizer\n",
        "model.compile(\n",
        "    optimizer='sgd',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# For RMSprop\n",
        "model.compile(\n",
        "    optimizer='rmsprop',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# For Adam\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n"
      ],
      "metadata": {
        "id": "K67V82L0Ky7y"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Full Example: Compare Optimizers Automatically\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "optimizers = ['sgd', 'rmsprop', 'adam']\n",
        "\n",
        "for opt_name in optimizers:\n",
        "    print(f\"\\nTraining with optimizer: {opt_name}\")\n",
        "\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=(28, 28)),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=opt_name,\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    model.fit(x_train, y_train, epochs=5, batch_size=128, verbose=0)\n",
        "    loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "    print(f\"Test accuracy with {opt_name}: {acc * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "4mncIN7yK0Mq",
        "outputId": "f2fb89f7-552d-4ca8-ae0a-bc90f9546f4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with optimizer: sgd\n",
            "Test accuracy with sgd: 91.91%\n",
            "\n",
            "Training with optimizer: rmsprop\n",
            "Test accuracy with rmsprop: 97.58%\n",
            "\n",
            "Training with optimizer: adam\n",
            "Test accuracy with adam: 97.23%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# 1. Load and normalize data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# 2. One-hot encode targets\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# 3. Build model with 5 hidden layers + Dropout for regularization\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# 4. Compile model with Adam optimizer and smaller learning rate\n",
        "adam = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=adam,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 5. Train model for 25 epochs with smaller batch size\n",
        "model.fit(x_train, y_train, epochs=25, batch_size=64, validation_split=0.1)\n",
        "\n",
        "# 6. Evaluate on test set\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f\"\\n✅ Test Accuracy: {test_acc * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "2pm8EKpLK5aQ",
        "outputId": "750662ce-755a-469d-f17d-5fb14532f418",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.7654 - loss: 0.7201 - val_accuracy: 0.9660 - val_loss: 0.1211\n",
            "Epoch 2/25\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.9451 - loss: 0.1848 - val_accuracy: 0.9713 - val_loss: 0.0992\n",
            "Epoch 3/25\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9586 - loss: 0.1390 - val_accuracy: 0.9730 - val_loss: 0.0870\n",
            "Epoch 4/25\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.9662 - loss: 0.1132 - val_accuracy: 0.9765 - val_loss: 0.0781\n",
            "Epoch 5/25\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.9694 - loss: 0.1030 - val_accuracy: 0.9800 - val_loss: 0.0730\n",
            "Epoch 6/25\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9704 - loss: 0.0956 - val_accuracy: 0.9773 - val_loss: 0.0741\n",
            "Epoch 7/25\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9741 - loss: 0.0832 - val_accuracy: 0.9800 - val_loss: 0.0706\n",
            "Epoch 8/25\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9784 - loss: 0.0732 - val_accuracy: 0.9810 - val_loss: 0.0725\n",
            "Epoch 9/25\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.9779 - loss: 0.0719 - val_accuracy: 0.9822 - val_loss: 0.0689\n",
            "Epoch 10/25\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9787 - loss: 0.0684 - val_accuracy: 0.9818 - val_loss: 0.0655\n",
            "Epoch 11/25\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9816 - loss: 0.0594 - val_accuracy: 0.9807 - val_loss: 0.0636\n",
            "Epoch 12/25\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9814 - loss: 0.0597 - val_accuracy: 0.9813 - val_loss: 0.0695\n",
            "Epoch 13/25\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9830 - loss: 0.0553 - val_accuracy: 0.9838 - val_loss: 0.0620\n",
            "Epoch 14/25\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9815 - loss: 0.0598 - val_accuracy: 0.9810 - val_loss: 0.0634\n",
            "Epoch 15/25\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9835 - loss: 0.0521 - val_accuracy: 0.9823 - val_loss: 0.0656\n",
            "Epoch 16/25\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9849 - loss: 0.0512 - val_accuracy: 0.9817 - val_loss: 0.0688\n",
            "Epoch 17/25\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9847 - loss: 0.0467 - val_accuracy: 0.9843 - val_loss: 0.0624\n",
            "Epoch 18/25\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9855 - loss: 0.0471 - val_accuracy: 0.9825 - val_loss: 0.0592\n",
            "Epoch 19/25\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9870 - loss: 0.0433 - val_accuracy: 0.9803 - val_loss: 0.0699\n",
            "Epoch 20/25\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.9866 - loss: 0.0432 - val_accuracy: 0.9827 - val_loss: 0.0678\n",
            "Epoch 21/25\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9868 - loss: 0.0412 - val_accuracy: 0.9808 - val_loss: 0.0737\n",
            "Epoch 22/25\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.9871 - loss: 0.0434 - val_accuracy: 0.9828 - val_loss: 0.0625\n",
            "Epoch 23/25\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.9878 - loss: 0.0406 - val_accuracy: 0.9845 - val_loss: 0.0576\n",
            "Epoch 24/25\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9895 - loss: 0.0365 - val_accuracy: 0.9817 - val_loss: 0.0719\n",
            "Epoch 25/25\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.9879 - loss: 0.0393 - val_accuracy: 0.9833 - val_loss: 0.0619\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9790 - loss: 0.0805\n",
            "\n",
            "✅ Test Accuracy: 98.20%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}